model,category,win_rate_h,win_rate_g,bias_score
alpaca-13b,coding,0.324468085106383,0.225,-0.09946808510638297
alpaca-13b,extraction,0.28846153846153844,0.225,-0.06346153846153843
alpaca-13b,humanities,0.22340425531914893,0.175,-0.04840425531914894
alpaca-13b,math,0.22137404580152673,0.29,0.06862595419847325
alpaca-13b,reasoning,0.3897058823529412,0.35,-0.0397058823529412
alpaca-13b,roleplay,0.3007246376811594,0.26,-0.040724637681159415
alpaca-13b,stem,0.22844827586206898,0.195,-0.03344827586206897
alpaca-13b,writing,0.32196969696969696,0.225,-0.09696969696969696
claude-v1,coding,0.6923076923076923,0.665,-0.027307692307692255
claude-v1,extraction,0.6830357142857143,0.725,0.041964285714285676
claude-v1,humanities,0.6598639455782312,0.775,0.11513605442176877
claude-v1,math,0.5398230088495575,0.535,-0.004823008849557442
claude-v1,reasoning,0.625,0.58,-0.04500000000000004
claude-v1,roleplay,0.6258278145695364,0.775,0.14917218543046362
claude-v1,stem,0.7184873949579832,0.83,0.11151260504201677
claude-v1,writing,0.6605504587155964,0.765,0.10444954128440365
gpt-3.5-turbo,coding,0.70625,0.685,-0.02124999999999999
gpt-3.5-turbo,extraction,0.706989247311828,0.755,0.04801075268817201
gpt-3.5-turbo,humanities,0.6450777202072538,0.525,-0.12007772020725382
gpt-3.5-turbo,math,0.7527472527472527,0.65,-0.10274725274725272
gpt-3.5-turbo,reasoning,0.5447761194029851,0.485,-0.059776119402985106
gpt-3.5-turbo,roleplay,0.6622222222222223,0.605,-0.057222222222222285
gpt-3.5-turbo,stem,0.5736196319018405,0.57,-0.0036196319018405587
gpt-3.5-turbo,writing,0.6279761904761905,0.595,-0.03297619047619049
gpt-4,coding,0.6705426356589147,0.875,0.2044573643410853
gpt-4,extraction,0.7520661157024794,0.765,0.012933884297520626
gpt-4,humanities,0.8104838709677419,0.93,0.11951612903225817
gpt-4,math,0.7637795275590551,0.88,0.11622047244094491
gpt-4,reasoning,0.6804511278195489,0.83,0.1495488721804511
gpt-4,roleplay,0.6851851851851852,0.765,0.07981481481481478
gpt-4,stem,0.8205128205128205,0.89,0.06948717948717953
gpt-4,writing,0.6538461538461539,0.925,0.2711538461538462
llama-13b,coding,0.1125,0.18,0.06749999999999999
llama-13b,extraction,0.14224137931034483,0.105,-0.03724137931034484
llama-13b,humanities,0.09797297297297297,0.05,-0.04797297297297297
llama-13b,math,0.24125874125874125,0.265,0.023741258741258764
llama-13b,reasoning,0.27205882352941174,0.265,-0.0070588235294117285
llama-13b,roleplay,0.12704918032786885,0.085,-0.04204918032786885
llama-13b,stem,0.06506849315068493,0.03,-0.03506849315068493
llama-13b,writing,0.19196428571428573,0.035,-0.15696428571428572
vicuna-13b-v1.2,coding,0.5074626865671642,0.37,-0.1374626865671642
vicuna-13b-v1.2,extraction,0.35251798561151076,0.425,0.07248201438848922
vicuna-13b-v1.2,humanities,0.5562913907284768,0.545,-0.011291390728476736
vicuna-13b-v1.2,math,0.41796875,0.38,-0.037968749999999996
vicuna-13b-v1.2,reasoning,0.47761194029850745,0.49,0.012388059701492538
vicuna-13b-v1.2,roleplay,0.45222929936305734,0.51,0.05777070063694267
vicuna-13b-v1.2,stem,0.553030303030303,0.485,-0.068030303030303
vicuna-13b-v1.2,writing,0.5229007633587787,0.455,-0.06790076335877865
