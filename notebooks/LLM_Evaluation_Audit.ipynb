{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fde9a4d",
   "metadata": {},
   "source": [
    "# **üî¨ LLM EVALUATION AUDIT: QUANTIFYING BIAS IN AI JUDGMENT**\n",
    "\n",
    "**Project Thesis:** This analysis investigates the credibility of LLM-as-a-Judge systems by comparing GPT-4 evaluations against human preferences. The goal is to establish a **Human-Validated Baseline** for task-specific model recommendations, necessary because AI evaluation lacks human alignment. \n",
    "\n",
    "**Core Argument Flow:**\n",
    "1. **The Problem:** Quantify the high disagreement rate (Credibility Gap).\n",
    "2. **The Solution:** Use human data to create a reliable Task Matrix.\n",
    "3. **The Conclusion:** Integrate SOTA claims against the human baseline to provide honest recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b71c21",
   "metadata": {},
   "source": [
    "### **1. Project Setup: Loading & Inspecting Human vs. AI Judgement Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128c924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human shape: (3355, 8)\n",
      "GPT4 pair shape: (2400, 8)\n",
      "Questions shape: (80, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW = DATA_DIR / \"raw\"\n",
    "PROCESS = DATA_DIR / \"processed\"\n",
    "\n",
    "# Load datasets\n",
    "df_h = pd.read_csv(RAW / \"human.csv\", low_memory=False)\n",
    "df_g = pd.read_csv(RAW / \"gpt4_pair.csv\", low_memory=False)\n",
    "\n",
    "# Load question JSONL\n",
    "import json\n",
    "questions = []\n",
    "with open(RAW / \"question.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        questions.append(json.loads(line))\n",
    "df_q = pd.DataFrame(questions)\n",
    "\n",
    "print(\"Human shape:\", df_h.shape)\n",
    "print(\"GPT4 pair shape:\", df_g.shape)\n",
    "print(\"Questions shape:\", df_q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fdcd8",
   "metadata": {},
   "source": [
    "#### **1.1 Show the first 5 rows of each dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e743144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   question_id     model_a          model_b   winner      judge  \\\n",
       " 0           81  alpaca-13b    gpt-3.5-turbo  model_b   author_2   \n",
       " 1           81  alpaca-13b    gpt-3.5-turbo  model_b   author_2   \n",
       " 2           81  alpaca-13b    gpt-3.5-turbo  model_b  expert_17   \n",
       " 3           81  alpaca-13b    gpt-3.5-turbo  model_b  expert_17   \n",
       " 4           81  alpaca-13b  vicuna-13b-v1.2  model_b   expert_0   \n",
       " \n",
       "                                       conversation_a  \\\n",
       " 0  [('Compose an engaging travel blog post about ...   \n",
       " 1  [('Compose an engaging travel blog post about ...   \n",
       " 2  [('Compose an engaging travel blog post about ...   \n",
       " 3  [('Compose an engaging travel blog post about ...   \n",
       " 4  [('Compose an engaging travel blog post about ...   \n",
       " \n",
       "                                       conversation_b  turn  \n",
       " 0  [('Compose an engaging travel blog post about ...     1  \n",
       " 1  [('Compose an engaging travel blog post about ...     2  \n",
       " 2  [('Compose an engaging travel blog post about ...     1  \n",
       " 3  [('Compose an engaging travel blog post about ...     2  \n",
       " 4  [('Compose an engaging travel blog post about ...     1  ,\n",
       "    question_id     model_a        model_b   winner      judge  \\\n",
       " 0           81  alpaca-13b      claude-v1  model_b  gpt4_pair   \n",
       " 1           81  alpaca-13b      claude-v1  model_b  gpt4_pair   \n",
       " 2           81  alpaca-13b  gpt-3.5-turbo  model_b  gpt4_pair   \n",
       " 3           81  alpaca-13b  gpt-3.5-turbo  model_b  gpt4_pair   \n",
       " 4           81  alpaca-13b          gpt-4  model_b  gpt4_pair   \n",
       " \n",
       "                                       conversation_a  \\\n",
       " 0  [('Compose an engaging travel blog post about ...   \n",
       " 1  [('Compose an engaging travel blog post about ...   \n",
       " 2  [('Compose an engaging travel blog post about ...   \n",
       " 3  [('Compose an engaging travel blog post about ...   \n",
       " 4  [('Compose an engaging travel blog post about ...   \n",
       " \n",
       "                                       conversation_b  turn  \n",
       " 0  [('Compose an engaging travel blog post about ...     1  \n",
       " 1  [('Compose an engaging travel blog post about ...     2  \n",
       " 2  [('Compose an engaging travel blog post about ...     1  \n",
       " 3  [('Compose an engaging travel blog post about ...     2  \n",
       " 4  [('Compose an engaging travel blog post about ...     1  ,\n",
       "    question_id category                                              turns  \\\n",
       " 0           81  writing  [Compose an engaging travel blog post about a ...   \n",
       " 1           82  writing  [Draft a professional email seeking your super...   \n",
       " 2           83  writing  [Imagine you are writing a blog post comparing...   \n",
       " 3           84  writing  [Write a persuasive email to convince your int...   \n",
       " 4           85  writing  [Describe a vivid and unique character, using ...   \n",
       " \n",
       "   reference  \n",
       " 0       NaN  \n",
       " 1       NaN  \n",
       " 2       NaN  \n",
       " 3       NaN  \n",
       " 4       NaN  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_h.head(), df_g.head(), df_q.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170a89d",
   "metadata": {},
   "source": [
    "#### **1.2 Show the column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef3bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question_id', 'model_a', 'model_b', 'winner', 'judge',\n",
      "       'conversation_a', 'conversation_b', 'turn'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['question_id', 'model_a', 'model_b', 'winner', 'judge',\n",
      "       'conversation_a', 'conversation_b', 'turn'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['question_id', 'category', 'turns', 'reference'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_h.columns, \"\\n\")\n",
    "print(df_g.columns, \"\\n\")\n",
    "print(df_q.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa7471",
   "metadata": {},
   "source": [
    "#### **1.3 Check the datatype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9a0efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id        int64\n",
      "model_a           object\n",
      "model_b           object\n",
      "winner            object\n",
      "judge             object\n",
      "conversation_a    object\n",
      "conversation_b    object\n",
      "turn               int64\n",
      "dtype: object\n",
      "question_id        int64\n",
      "model_a           object\n",
      "model_b           object\n",
      "winner            object\n",
      "judge             object\n",
      "conversation_a    object\n",
      "conversation_b    object\n",
      "turn               int64\n",
      "dtype: object\n",
      "question_id     int64\n",
      "category       object\n",
      "turns          object\n",
      "reference      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_h.dtypes)\n",
    "print(df_g.dtypes)\n",
    "print(df_q.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425113ca",
   "metadata": {},
   "source": [
    "### **2. Data Cleaning & Winner Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb1263",
   "metadata": {},
   "source": [
    "#### **2.1 Standardize Column Names and question_id types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b62c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(question_id       object\n",
       " model_a           object\n",
       " model_b           object\n",
       " winner            object\n",
       " judge             object\n",
       " conversation_a    object\n",
       " conversation_b    object\n",
       " turn               int64\n",
       " dtype: object,\n",
       " question_id       object\n",
       " model_a           object\n",
       " model_b           object\n",
       " winner            object\n",
       " judge             object\n",
       " conversation_a    object\n",
       " conversation_b    object\n",
       " turn               int64\n",
       " dtype: object,\n",
       " question_id    object\n",
       " category       object\n",
       " turns          object\n",
       " reference      object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Strip whitespace from all column names ---\n",
    "df_h.columns = df_h.columns.str.strip()\n",
    "df_g.columns = df_g.columns.str.strip()\n",
    "df_q.columns = df_q.columns.str.strip()\n",
    "\n",
    "# --- Convert question_id to string in all datasets for safe merging ---\n",
    "df_h[\"question_id\"] = df_h[\"question_id\"].astype(str)\n",
    "df_g[\"question_id\"] = df_g[\"question_id\"].astype(str)\n",
    "df_q[\"question_id\"] = df_q[\"question_id\"].astype(str)\n",
    "\n",
    "# Verify\n",
    "df_h.dtypes, df_g.dtypes, df_q.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e0029",
   "metadata": {},
   "source": [
    "#### **2.2 Resolve and Normalize Winner Column (Assign Model Name or Tie)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e436b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_a",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_b",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "winner",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "winner_model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "is_tie",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "11c01b58-c86a-45e3-961c-350a5af3f300",
       "rows": [
        [
         "0",
         "81",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "model_b",
         "gpt-3.5-turbo",
         "False"
        ],
        [
         "1",
         "81",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "model_b",
         "gpt-3.5-turbo",
         "False"
        ],
        [
         "2",
         "81",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "model_b",
         "gpt-3.5-turbo",
         "False"
        ],
        [
         "3",
         "81",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "model_b",
         "gpt-3.5-turbo",
         "False"
        ],
        [
         "4",
         "81",
         "alpaca-13b",
         "vicuna-13b-v1.2",
         "model_b",
         "vicuna-13b-v1.2",
         "False"
        ],
        [
         "5",
         "81",
         "alpaca-13b",
         "vicuna-13b-v1.2",
         "tie",
         null,
         "True"
        ],
        [
         "6",
         "81",
         "claude-v1",
         "alpaca-13b",
         "model_a",
         "claude-v1",
         "False"
        ],
        [
         "7",
         "81",
         "claude-v1",
         "alpaca-13b",
         "model_a",
         "claude-v1",
         "False"
        ],
        [
         "8",
         "81",
         "claude-v1",
         "llama-13b",
         "model_a",
         "claude-v1",
         "False"
        ],
        [
         "9",
         "81",
         "claude-v1",
         "llama-13b",
         "tie",
         null,
         "True"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_model</th>\n",
       "      <th>is_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>model_b</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>tie</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>llama-13b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>llama-13b</td>\n",
       "      <td>tie</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id     model_a          model_b   winner     winner_model  is_tie\n",
       "0          81  alpaca-13b    gpt-3.5-turbo  model_b    gpt-3.5-turbo   False\n",
       "1          81  alpaca-13b    gpt-3.5-turbo  model_b    gpt-3.5-turbo   False\n",
       "2          81  alpaca-13b    gpt-3.5-turbo  model_b    gpt-3.5-turbo   False\n",
       "3          81  alpaca-13b    gpt-3.5-turbo  model_b    gpt-3.5-turbo   False\n",
       "4          81  alpaca-13b  vicuna-13b-v1.2  model_b  vicuna-13b-v1.2   False\n",
       "5          81  alpaca-13b  vicuna-13b-v1.2      tie             None    True\n",
       "6          81   claude-v1       alpaca-13b  model_a        claude-v1   False\n",
       "7          81   claude-v1       alpaca-13b  model_a        claude-v1   False\n",
       "8          81   claude-v1        llama-13b  model_a        claude-v1   False\n",
       "9          81   claude-v1        llama-13b      tie             None    True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resolve_winner(df):\n",
    "    def pick(row):\n",
    "        w = str(row.get('winner', '')).lower().strip()\n",
    "        a = row.get('model_a')\n",
    "        b = row.get('model_b')\n",
    "\n",
    "        # Direct label cases\n",
    "        if w in ['model_a', 'a', 'a wins', 'a_win', 'model a']:\n",
    "            return a\n",
    "        if w in ['model_b', 'b', 'b wins', 'b_win', 'model b']:\n",
    "            return b\n",
    "\n",
    "        # If model name appears inside winner string\n",
    "        if isinstance(a, str) and a.lower() in w:\n",
    "            return a\n",
    "        if isinstance(b, str) and b.lower() in w:\n",
    "            return b\n",
    "\n",
    "        # Anything else ‚Üí treat as tie\n",
    "        return None\n",
    "\n",
    "    df['winner_model'] = df.apply(pick, axis=1)\n",
    "    df['is_tie'] = df['winner'].astype(str).str.contains(\"tie\", case=False, na=False)\n",
    "    return df\n",
    "\n",
    "df_h = resolve_winner(df_h)\n",
    "df_g = resolve_winner(df_g)\n",
    "\n",
    "df_h[['question_id','model_a','model_b','winner','winner_model','is_tie']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7d5b8",
   "metadata": {},
   "source": [
    "#### **2.3 Merge Category metadata into judgment datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7843272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Categories in human: 0\n",
      "Number of Missing Categories in GPT: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    question_id     model_a    model_b   winner      judge  \\\n",
       " 735          99  alpaca-13b  claude-v1  model_b  expert_17   \n",
       " \n",
       "                                         conversation_a  \\\n",
       " 735  [('Suppose you are a mathematician and poet. Y...   \n",
       " \n",
       "                                         conversation_b  turn winner_model  \\\n",
       " 735  [('Suppose you are a mathematician and poet. Y...     1    claude-v1   \n",
       " \n",
       "      is_tie  category  \n",
       " 735   False  roleplay  ,\n",
       "      question_id model_a    model_b              winner      judge  \\\n",
       " 1182         120   gpt-4  claude-v1  tie (inconsistent)  gpt4_pair   \n",
       " \n",
       "                                          conversation_a  \\\n",
       " 1182  [('Given that f(x) = 4x^3 - 9x - 14, find the ...   \n",
       " \n",
       "                                          conversation_b  turn winner_model  \\\n",
       " 1182  [('Given that f(x) = 4x^3 - 9x - 14, find the ...     1         None   \n",
       " \n",
       "       is_tie category  \n",
       " 1182    True     math  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q_small = df_q[[\"question_id\", \"category\",]].drop_duplicates()\n",
    "\n",
    "df_h = df_h.merge(df_q_small, on=\"question_id\", how=\"left\")\n",
    "df_g = df_g.merge(df_q_small, on=\"question_id\", how=\"left\")\n",
    "\n",
    "print(\"Number of Missing Categories in human:\", df_h[\"category\"].isnull().sum())\n",
    "print(\"Number of Missing Categories in GPT:\", df_g[\"category\"].isnull().sum())\n",
    "\n",
    "df_h.sample(), df_g.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e1ece",
   "metadata": {},
   "source": [
    "### **3. The Crisis: Quantifying the Human-AI Credibility Gap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0850674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "appearances",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wins",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tie_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "effective_wins",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "win_rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2478f7c8-fbb9-4f95-8b47-26f0bedf86fe",
       "rows": [
        [
         "0",
         "alpaca-13b",
         "coding",
         "94",
         "20",
         "21",
         "30.5",
         "0.324468085106383"
        ],
        [
         "1",
         "gpt-3.5-turbo",
         "coding",
         "160",
         "89",
         "48",
         "113.0",
         "0.70625"
        ],
        [
         "2",
         "vicuna-13b-v1.2",
         "coding",
         "134",
         "49",
         "38",
         "68.0",
         "0.5074626865671642"
        ],
        [
         "3",
         "claude-v1",
         "coding",
         "117",
         "65",
         "32",
         "81.0",
         "0.6923076923076923"
        ],
        [
         "4",
         "llama-13b",
         "coding",
         "160",
         "3",
         "30",
         "18.0",
         "0.1125"
        ],
        [
         "5",
         "gpt-4",
         "coding",
         "129",
         "69",
         "35",
         "86.5",
         "0.6705426356589147"
        ],
        [
         "6",
         "alpaca-13b",
         "extraction",
         "130",
         "22",
         "31",
         "37.5",
         "0.28846153846153844"
        ],
        [
         "7",
         "gpt-3.5-turbo",
         "extraction",
         "186",
         "105",
         "53",
         "131.5",
         "0.706989247311828"
        ],
        [
         "8",
         "vicuna-13b-v1.2",
         "extraction",
         "139",
         "35",
         "28",
         "49.0",
         "0.35251798561151076"
        ],
        [
         "9",
         "claude-v1",
         "extraction",
         "112",
         "58",
         "37",
         "76.5",
         "0.6830357142857143"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>category</th>\n",
       "      <th>appearances</th>\n",
       "      <th>wins</th>\n",
       "      <th>tie_count</th>\n",
       "      <th>effective_wins</th>\n",
       "      <th>win_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>coding</td>\n",
       "      <td>94</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.324468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>coding</td>\n",
       "      <td>160</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>coding</td>\n",
       "      <td>134</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.507463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-v1</td>\n",
       "      <td>coding</td>\n",
       "      <td>117</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-13b</td>\n",
       "      <td>coding</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>coding</td>\n",
       "      <td>129</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>86.5</td>\n",
       "      <td>0.670543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>extraction</td>\n",
       "      <td>130</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>extraction</td>\n",
       "      <td>186</td>\n",
       "      <td>105</td>\n",
       "      <td>53</td>\n",
       "      <td>131.5</td>\n",
       "      <td>0.706989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>extraction</td>\n",
       "      <td>139</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.352518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>claude-v1</td>\n",
       "      <td>extraction</td>\n",
       "      <td>112</td>\n",
       "      <td>58</td>\n",
       "      <td>37</td>\n",
       "      <td>76.5</td>\n",
       "      <td>0.683036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model    category  appearances  wins  tie_count  effective_wins  \\\n",
       "0       alpaca-13b      coding           94    20         21            30.5   \n",
       "1    gpt-3.5-turbo      coding          160    89         48           113.0   \n",
       "2  vicuna-13b-v1.2      coding          134    49         38            68.0   \n",
       "3        claude-v1      coding          117    65         32            81.0   \n",
       "4        llama-13b      coding          160     3         30            18.0   \n",
       "5            gpt-4      coding          129    69         35            86.5   \n",
       "6       alpaca-13b  extraction          130    22         31            37.5   \n",
       "7    gpt-3.5-turbo  extraction          186   105         53           131.5   \n",
       "8  vicuna-13b-v1.2  extraction          139    35         28            49.0   \n",
       "9        claude-v1  extraction          112    58         37            76.5   \n",
       "\n",
       "   win_rate  \n",
       "0  0.324468  \n",
       "1  0.706250  \n",
       "2  0.507463  \n",
       "3  0.692308  \n",
       "4  0.112500  \n",
       "5  0.670543  \n",
       "6  0.288462  \n",
       "7  0.706989  \n",
       "8  0.352518  \n",
       "9  0.683036  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_model_category_stats(df):\n",
    "    results = []\n",
    "\n",
    "    # All model names from model_a and model_b\n",
    "    models = pd.unique(df[['model_a','model_b']].values.ravel())\n",
    "    models = [m for m in models if pd.notnull(m)]\n",
    "\n",
    "    # Group by category\n",
    "    for cat, group in df.groupby('category'):\n",
    "        for m in models:\n",
    "            # appearances\n",
    "            appear = ((group['model_a'] == m) | (group['model_b'] == m)).sum()\n",
    "            if appear == 0:\n",
    "                continue\n",
    "\n",
    "            # wins\n",
    "            wins = (group['winner_model'] == m).sum()\n",
    "\n",
    "            # ties\n",
    "            ties = group['is_tie'] & ((group['model_a'] == m) | (group['model_b'] == m))\n",
    "            tie_count = ties.sum()\n",
    "\n",
    "            # effective wins\n",
    "            eff_wins = wins + 0.5 * tie_count\n",
    "\n",
    "            # win rate\n",
    "            win_rate = eff_wins / appear\n",
    "\n",
    "            results.append({\n",
    "                \"model\": m,\n",
    "                \"category\": cat,\n",
    "                \"appearances\": int(appear),\n",
    "                \"wins\": int(wins),\n",
    "                \"tie_count\": int(tie_count),\n",
    "                \"effective_wins\": eff_wins,\n",
    "                \"win_rate\": win_rate\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "stats_h = compute_model_category_stats(df_h)\n",
    "stats_g = compute_model_category_stats(df_g)\n",
    "\n",
    "# Preview the human-based metrics\n",
    "stats_h.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102e3a9",
   "metadata": {},
   "source": [
    "#### **3.1 Best Model by Category: The Flawed AI Leaderboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2ec82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model according to Human Judgement:\n",
      "            model    category  appearances  wins  tie_count  effective_wins  \\\n",
      "0  gpt-3.5-turbo      coding          160    89         48           113.0   \n",
      "1          gpt-4  extraction          121    75         32            91.0   \n",
      "2          gpt-4  humanities          124    90         21           100.5   \n",
      "3          gpt-4        math          127    82         30            97.0   \n",
      "4          gpt-4   reasoning          133    64         53            90.5   \n",
      "5          gpt-4    roleplay          135    83         19            92.5   \n",
      "6          gpt-4        stem          156   118         20           128.0   \n",
      "7      claude-v1     writing          109    63         18            72.0   \n",
      "\n",
      "   win_rate  \n",
      "0  0.706250  \n",
      "1  0.752066  \n",
      "2  0.810484  \n",
      "3  0.763780  \n",
      "4  0.680451  \n",
      "5  0.685185  \n",
      "6  0.820513  \n",
      "7  0.660550  \n",
      "Best Model according to GPT Judgement:\n",
      "        model    category  appearances  wins  tie_count  effective_wins  \\\n",
      "0      gpt-4      coding          100    78         19            87.5   \n",
      "1      gpt-4  extraction          100    61         31            76.5   \n",
      "2      gpt-4  humanities          100    87         12            93.0   \n",
      "3      gpt-4        math          100    77         22            88.0   \n",
      "4      gpt-4   reasoning          100    72         22            83.0   \n",
      "5  claude-v1    roleplay          100    72         11            77.5   \n",
      "6      gpt-4        stem          100    82         14            89.0   \n",
      "7      gpt-4     writing          100    91          3            92.5   \n",
      "\n",
      "   win_rate  \n",
      "0     0.875  \n",
      "1     0.765  \n",
      "2     0.930  \n",
      "3     0.880  \n",
      "4     0.830  \n",
      "5     0.775  \n",
      "6     0.890  \n",
      "7     0.925  \n"
     ]
    }
   ],
   "source": [
    "sorted_stats_h = stats_h.sort_values(['category', 'win_rate'], ascending=[True, False])\n",
    "sorted_stats_g = stats_g.sort_values(['category', 'win_rate'], ascending=[True, False])\n",
    "\n",
    "best_per_category_human = sorted_stats_h.groupby('category').head(1)\n",
    "best_per_category_GPT = sorted_stats_g.groupby('category').head(1)\n",
    "\n",
    "print(\"Best Model according to Human Judgement:\\n\", best_per_category_human.reset_index(drop=True))\n",
    "print(\"Best Model according to GPT Judgement:\\n\", best_per_category_GPT.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fa2e5",
   "metadata": {},
   "source": [
    "##### **‚ùå KEY BIAS FINDING: GPT-4 Self-Preference vs. Human Preference**\n",
    "\n",
    "- **GPT-4 Judge Bias:** The GPT-4 judge consistently awards GPT-4 output win rates far exceeding human judgement (e.g., GPT-4 win rate in Humanities: **93.0%** by AI vs. **81.0%** by Human).\n",
    "- **Human Disagreement:** Humans rate **GPT-3.5-turbo** as the best model for **Coding** (0.706 win rate), while the GPT-4 judge ignores this preference and favors itself (0.875 win rate).\n",
    "- **Conclusion:** The AI-generated leaderboard is compromised by evaluator bias, necessitating a **human-validated baseline** for credible task recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93662cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Models Per Category according to Human:\n",
      "            model    category  appearances  wins  tie_count  effective_wins  \\\n",
      "0  gpt-3.5-turbo      coding          160    89         48           113.0   \n",
      "1          gpt-4  extraction          121    75         32            91.0   \n",
      "2          gpt-4  humanities          124    90         21           100.5   \n",
      "3          gpt-4        math          127    82         30            97.0   \n",
      "4          gpt-4   reasoning          133    64         53            90.5   \n",
      "5          gpt-4    roleplay          135    83         19            92.5   \n",
      "6          gpt-4        stem          156   118         20           128.0   \n",
      "7      claude-v1     writing          109    63         18            72.0   \n",
      "\n",
      "   win_rate  \n",
      "0  0.706250  \n",
      "1  0.752066  \n",
      "2  0.810484  \n",
      "3  0.763780  \n",
      "4  0.680451  \n",
      "5  0.685185  \n",
      "6  0.820513  \n",
      "7  0.660550  \n",
      "Top 5 Models Per Category according to GPT:\n",
      "        model    category  appearances  wins  tie_count  effective_wins  \\\n",
      "0      gpt-4      coding          100    78         19            87.5   \n",
      "1      gpt-4  extraction          100    61         31            76.5   \n",
      "2      gpt-4  humanities          100    87         12            93.0   \n",
      "3      gpt-4        math          100    77         22            88.0   \n",
      "4      gpt-4   reasoning          100    72         22            83.0   \n",
      "5  claude-v1    roleplay          100    72         11            77.5   \n",
      "6      gpt-4        stem          100    82         14            89.0   \n",
      "7      gpt-4     writing          100    91          3            92.5   \n",
      "\n",
      "   win_rate  \n",
      "0     0.875  \n",
      "1     0.765  \n",
      "2     0.930  \n",
      "3     0.880  \n",
      "4     0.830  \n",
      "5     0.775  \n",
      "6     0.890  \n",
      "7     0.925  \n"
     ]
    }
   ],
   "source": [
    "top5_models_human = sorted_stats_h.groupby('category').head(1).reset_index(drop=True)\n",
    "top5_models_GPT = sorted_stats_g.groupby('category').head(1).reset_index(drop=True)\n",
    "\n",
    "print(\"Top 5 Models Per Category according to Human:\\n\", top5_models_human)\n",
    "print(\"Top 5 Models Per Category according to GPT:\\n\", top5_models_GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a85b1",
   "metadata": {},
   "source": [
    "#### **3.2 The Flawed Metric: Measuring Agreement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67faaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "winner_h",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "winner_g",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "9fba48aa-3724-4211-abd4-499222c49c26",
       "rows": [
        [
         "0",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "1",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "2",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "3",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "4",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "5",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "6",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "7",
         "81",
         "writing",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo"
        ],
        [
         "8",
         "81",
         "writing",
         "vicuna-13b-v1.2",
         "vicuna-13b-v1.2"
        ],
        [
         "9",
         "81",
         "writing",
         "vicuna-13b-v1.2",
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>winner_h</th>\n",
       "      <th>winner_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id category         winner_h         winner_g\n",
       "0          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "1          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "2          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "3          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "4          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "5          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "6          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "7          81  writing    gpt-3.5-turbo    gpt-3.5-turbo\n",
       "8          81  writing  vicuna-13b-v1.2  vicuna-13b-v1.2\n",
       "9          81  writing  vicuna-13b-v1.2             None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Renaming columns to avoid conflict during merge ---\n",
    "\n",
    "df_h.rename(columns={'winner_model': 'winner_h'}, inplace=True)\n",
    "df_g.rename(columns={'winner_model': 'winner_g'}, inplace=True)\n",
    "\n",
    "# --- Merging human and GPT evaluations ---\n",
    "df_compare = df_h.merge(\n",
    "    df_g,\n",
    "    on=[\"question_id\", \"model_a\", \"model_b\"]\n",
    ")\n",
    "\n",
    "df_compare.rename(columns={'category_x' : 'category'}, inplace=True)\n",
    "\n",
    "df_compare[[\"question_id\", \"category\", \"winner_h\", \"winner_g\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a2858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement Rate: 0.53 Disagreement Rate: 0.47\n"
     ]
    }
   ],
   "source": [
    "df_compare[\"agree\"] = df_compare[\"winner_h\"] == df_compare[\"winner_g\"]\n",
    "\n",
    "agreement_rate = df_compare[\"agree\"].mean()\n",
    "disagreement_rate = 1 - agreement_rate\n",
    "\n",
    "print(\"Agreement Rate:\", round(agreement_rate, 2), \"Disagreement Rate:\", round(disagreement_rate, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b060b",
   "metadata": {},
   "source": [
    "##### **‚ùå The 53% Credibility Gap**\n",
    "\n",
    "The 53% overall agreement rate reveals that LLM-as-a-Judge systems **disagree with human preference nearly half the time**. This establishes a profound credibility crisis for any modern benchmark relying solely on AI judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a028dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category  agreement_rate  disagreement_rate\n",
      "0        stem        0.706522           0.293478\n",
      "1  extraction        0.630769           0.369231\n",
      "2    roleplay        0.606838           0.393162\n",
      "3  humanities        0.597166           0.402834\n",
      "4     writing        0.576923           0.423077\n",
      "5      coding        0.490000           0.510000\n",
      "6        math        0.362445           0.637555\n",
      "7   reasoning        0.314103           0.685897\n"
     ]
    }
   ],
   "source": [
    "agreement_rate_category = df_compare.groupby('category')[\"agree\"].mean()\n",
    "disagreement_rate_category = 1 - agreement_rate_category\n",
    "\n",
    "df_agree_powerbi = pd.DataFrame({\n",
    "    \"category\": agreement_rate_category.index,\n",
    "    \"agreement_rate\": agreement_rate_category.values,\n",
    "    \"disagreement_rate\": disagreement_rate_category.values\n",
    "})\n",
    "\n",
    "df_agree_powerbi = df_agree_powerbi.sort_values('agreement_rate', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_agree_powerbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aed3c9",
   "metadata": {},
   "source": [
    "##### **‚ö†Ô∏è The Unreliable Categories**\n",
    "\n",
    "- **Reasoning has extremely low agreement (31%):** This is the biggest failure point. LLM-judged reasoning benchmarks are highly unreliable. \n",
    "- **Math agreement is very low (36%):** LLM judges are likely rewarding verbose explanations; humans reward demonstrable correctness. \n",
    "- **Coding shows high disagreement (~49%):** LLM judges focus on style, leading to bias (e.g., favoring GPT-4 code); humans prioritize functional correctness. \n",
    "- **STEM & Extraction show high agreement (70%+):** Structured, fact-based tasks are where LLM judges align most closely with human preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78ec84",
   "metadata": {},
   "source": [
    "### **4. The Solution: Establishing the Human-Validated Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea6b6b",
   "metadata": {},
   "source": [
    "#### **4.1 Human-Validated Task Matrix**\n",
    "\n",
    "The data below represents the most credible ranking of models for these specific task categories, filtered by human preference, not AI bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f60ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: human_model_rankings_for_powerbi.csv\n"
     ]
    }
   ],
   "source": [
    "# Exporting the Human-Validated Rankings for the PowerBI Baseline\n",
    "df_rankings_human = stats_h[['model', 'category', 'win_rate', 'appearances']].sort_values(by=['category', 'win_rate'], ascending=[True, False])\n",
    "df_rankings_human.to_csv(PROCESS / \"human_model_rankings_for_powerbi.csv\", index=False)\n",
    "\n",
    "print(\"Exported: human_model_rankings_for_powerbi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be46597",
   "metadata": {},
   "source": [
    "#### **4.2 Qualitative Disagreement Analysis & Bias Types**\n",
    "\n",
    "The qualitative analysis of disagreement cases identifies specific, known biases in the GPT-4 judge:\n",
    "\n",
    "- **Self-Bias:** GPT-4 favors its own style, often giving itself the win even when human judges see outputs as equally flawed (e.g., in Coding and STEM).\n",
    "- **Style Bias:** The LLM judge favors verbose and formal responses, missing the human preference for empathy and personality (e.g., in Roleplay and Writing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "259d4e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_a",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_b",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "winner_h",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "winner_g",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "cc17ea19-2a90-4e75-9e97-79d2600ee840",
       "rows": [
        [
         "9",
         "81",
         "writing",
         "alpaca-13b",
         "vicuna-13b-v1.2",
         "vicuna-13b-v1.2",
         null
        ],
        [
         "368",
         "91",
         "roleplay",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "alpaca-13b",
         "gpt-3.5-turbo"
        ],
        [
         "833",
         "101",
         "reasoning",
         "alpaca-13b",
         "gpt-3.5-turbo",
         "gpt-3.5-turbo",
         null
        ],
        [
         "1300",
         "111",
         "math",
         "alpaca-13b",
         "claude-v1",
         null,
         null
        ],
        [
         "1774",
         "121",
         "coding",
         "alpaca-13b",
         "gpt-4",
         null,
         "gpt-4"
        ],
        [
         "2158",
         "131",
         "extraction",
         "gpt-3.5-turbo",
         "gpt-4",
         null,
         null
        ],
        [
         "2548",
         "141",
         "stem",
         "gpt-3.5-turbo",
         "gpt-4",
         null,
         "gpt-4"
        ],
        [
         "3018",
         "151",
         "humanities",
         "gpt-4",
         "claude-v1",
         "claude-v1",
         "gpt-4"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner_h</th>\n",
       "      <th>winner_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>writing</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>vicuna-13b-v1.2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>91</td>\n",
       "      <td>roleplay</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>101</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>111</td>\n",
       "      <td>math</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>121</td>\n",
       "      <td>coding</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>131</td>\n",
       "      <td>extraction</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>141</td>\n",
       "      <td>stem</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>151</td>\n",
       "      <td>humanities</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>claude-v1</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_id    category        model_a          model_b         winner_h  \\\n",
       "9             81     writing     alpaca-13b  vicuna-13b-v1.2  vicuna-13b-v1.2   \n",
       "368           91    roleplay     alpaca-13b    gpt-3.5-turbo       alpaca-13b   \n",
       "833          101   reasoning     alpaca-13b    gpt-3.5-turbo    gpt-3.5-turbo   \n",
       "1300         111        math     alpaca-13b        claude-v1             None   \n",
       "1774         121      coding     alpaca-13b            gpt-4             None   \n",
       "2158         131  extraction  gpt-3.5-turbo            gpt-4             None   \n",
       "2548         141        stem  gpt-3.5-turbo            gpt-4             None   \n",
       "3018         151  humanities          gpt-4        claude-v1        claude-v1   \n",
       "\n",
       "           winner_g  \n",
       "9              None  \n",
       "368   gpt-3.5-turbo  \n",
       "833            None  \n",
       "1300           None  \n",
       "1774          gpt-4  \n",
       "2158           None  \n",
       "2548          gpt-4  \n",
       "3018          gpt-4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disagree = df_compare[df_compare['winner_h'] != df_compare['winner_g']]\n",
    "\n",
    "df_disagree[[\"question_id\", \"category\", \"model_a\", \"model_b\",\"winner_h\", \"winner_g\",]].groupby('category').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4892c",
   "metadata": {},
   "source": [
    "### **5. Conclusion & Forward-Looking Task Recommendations (SOTA Integration)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b4b7c",
   "metadata": {},
   "source": [
    "#### **5.1 The Final Recommendation Thesis**\n",
    "\n",
    "**The Problem:** SOTA LLMs (Gemini 3 Pro, GPT-5.1) are ranked using AI judges, whose credibility is invalidated by the 53% agreement gap demonstrated in this report.\n",
    "\n",
    "**The Solution:** All recommendations must be filtered through our human-validated baseline, then updated with competitive, fact-based SOTA claims. The final choice is a trade-off between **Proven Human Preference** (Baseline) and **Raw Capability** (SOTA).\n",
    "\n",
    "| Category | Proven Human Preference (Baseline) | New SOTA Model Claim (2025) | Justification/Context for Use |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Writing/Creative** | Claude-v1 (Best Style) | Claude 3.7 Sonnet / GPT-5.1 | Anthropic models maintain their lead for human-like prose and safety. Choose Claude for sensitive or narrative tasks. |\n",
    "| **Coding/Agentic** | GPT-3.5-turbo (Best Value) | Gemini 3 Pro / Claude 3.7 Sonnet | Gemini 3 Pro leads LiveCodeBench Elo (2,439). Claude 3.7 Sonnet has superior real-world performance on SWE-Bench. Choose these for competitive development or complex agents. |\n",
    "| **Reasoning/Math** | GPT-4 (Highest Win-Rate) | Gemini 3 Pro | Gemini 3 Pro leads in novel reasoning (ARC-AGI-2) and symbolic math (AIME), making it the technical SOTA, but be aware of judge bias inflation. |\n",
    "| **Extraction & STEM** | GPT-4 (Highest Win-Rate) | GPT-5.1 / Gemini 3 Pro | Use the current frontier model for fact-based and technical knowledge retrieval. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b4a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: sota_integration_verified_for_powerbi.csv\n"
     ]
    }
   ],
   "source": [
    "df_sota_integration_verified = pd.DataFrame({\n",
    "    'Category': ['Writing/Creative', 'Coding/Agentic', 'Reasoning/Math', 'Extraction & STEM'],\n",
    "    'Human_Validated_Baseline': ['Claude-v1 (Best Style)', 'GPT-3.5-turbo (Best Value)', 'GPT-4 (Highest Win-Rate)', 'GPT-4 (Highest Win-Rate)'],\n",
    "    'New_SOTA_Recommendation': ['Claude 3.7 Sonnet / GPT-5.1', 'Gemini 3 Pro / Claude 3.7 Sonnet', 'Gemini 3 Pro', 'GPT-5.1 / Gemini 3 Pro'],\n",
    "    'SOTA_Justification_and_Data': [\n",
    "        'Anthropic models still excel at coherent, safe, and long-form human-like dialogue, maintaining their lead in stylistic preference.',\n",
    "        'Gemini 3 Pro leads LiveCodeBench Elo (2,439). Claude 3.7 Sonnet has superior real-world performance on SWE-Bench.',\n",
    "        'Gemini 3 Pro leads in novel reasoning (ARC-AGI-2) and symbolic math (AIME), making it the technical SOTA.',\n",
    "        'GPT-5.1 shows strong general performance (GPQA 88.1%). Gemini 3 Pro excels due to native multimodality (diagrams/charts).'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Export the final verified table for PowerBI\n",
    "df_sota_integration_verified.to_csv(PROCESS / \"sota_integration_verified_for_powerbi.csv\", index=False)\n",
    "print(\"Exported: sota_integration_verified_for_powerbi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ad4d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All final data exports confirmed.\n"
     ]
    }
   ],
   "source": [
    "# FINAL EXPORT CONFIRMATION (Ensuring all visual data is ready)\n",
    "df_agree_powerbi.to_csv(PROCESS / \"agreement_rates_for_powerbi.csv\", index=False)\n",
    "df_compare.to_csv(PROCESS / \"human_gpt_comparison.csv\", index=False)\n",
    "df_disagree = df_compare[df_compare['winner_h'] != df_compare['winner_g']]\n",
    "df_disagree.to_csv(PROCESS / \"disagreement_examples.csv\", index=False)\n",
    "\n",
    "print(\"All final data exports confirmed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b67d863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: bias_metrics_for_powerbi.csv (For Page 1 Bias Heatmap)\n"
     ]
    }
   ],
   "source": [
    "# 1. Select and rename win rates from both judge tables\n",
    "df_bias = stats_h[['model', 'category', 'win_rate']].rename(columns={'win_rate': 'win_rate_h'}).copy()\n",
    "df_bias = df_bias.merge(\n",
    "    stats_g[['model', 'category', 'win_rate']].rename(columns={'win_rate': 'win_rate_g'}),\n",
    "    on=['model', 'category'],\n",
    "    how='outer'\n",
    ").fillna(0) \n",
    "\n",
    "# 2. Calculate the Bias: GPT Win Rate - Human Win Rate\n",
    "df_bias['bias_score'] = df_bias['win_rate_g'] - df_bias['win_rate_h']\n",
    "\n",
    "# 3. Export for Page 1 Bias Heatmap\n",
    "df_bias.to_csv(PROCESS / \"bias_metrics_for_powerbi.csv\", index=False)\n",
    "print(f\"Exported: bias_metrics_for_powerbi.csv (For Page 1 Bias Heatmap)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
